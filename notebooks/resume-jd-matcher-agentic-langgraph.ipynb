{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b82be6f",
   "metadata": {},
   "source": [
    "# <center>**`Project Details`**</center>\n",
    "\n",
    "#### **Purpose**:\n",
    "\n",
    "Tis project goal is matching a resume to a job description. A poorly aligned resume can lead to missed opportunities, even when the candidate is a strong fit. The project aims to showcase how we can make use of AI agents to help applicants tailor their resumes more strategically, uncover hidden gaps, and present a stronger case to recruiters — all with minimal effort.\n",
    "\n",
    "A [Github repo](https://github.com/vikrambhat2/MultiAgents-with-CrewAI-ResumeJDMatcher) tackling this project exist and our goal will be to improve it by seperating the **Backend** and the **Frontend** logics.\n",
    "\n",
    "##### **Why Split**?\n",
    "\n",
    " - The backend will hold business logic, agent orchestration, model calls, state, data processing, API endpoints, while the frontend focuses on the UI/UX, user interaction, session management, file upload, displaying results.\n",
    " - Scalability: Backend can scale independently of UI (and can even serve other clients)\n",
    " - Security: Sensitive logic, API keys, and resource-intensive processing are kept server-side\n",
    " - Performance: Streamlit remains snappy, while heavy lifting is offloaded to backend\n",
    " \n",
    "##### **Responsibilities**\n",
    "\n",
    "  - *<u>Backend</u>*: \n",
    "    - Expose REST API endpoints:\n",
    "        - `/match`: Accepts resume + JD, returns match results and insights.\n",
    "        - `/enhance`: Accepts resume + JD, returns resume improvement suggestions.\n",
    "        - `/cover-letter`: Accepts resume + JD, returns a cover letter.\n",
    "    - Agent orchestration: All CrewAI workflows run here.\n",
    "    - Input validation, error handling.\n",
    "    - PDF/text parsing if desired (or can also be handled in frontend, see below).\n",
    "    - Optional: Authentication, user/session management, logging, monitoring.\n",
    "    - Optional: Serve as an async queue for heavy jobs if latency is an issue (using Celery/RQ, etc.).\n",
    " \n",
    " - *<u>Frontend</u>(Streamlit)*\n",
    "    - UI for uploading files, entering/pasting text.\n",
    "    - Visualization: Render reports, scores, enhanced resume, cover letter, etc.\n",
    "    - API client: Handles all interaction with FastAPI backend.\n",
    "    - Light preprocessing: E.g., local PDF parsing if you want to send plain text to backend (saves bandwidth).\n",
    "    - Session/user state, feedback, download links, etc.\n",
    "\n",
    "Here is how the system works (Flow):\n",
    "\n",
    " 1. User uploads resume & JD (PDF or text) → Streamlit UI\n",
    "\n",
    " 2. Frontend extracts or passes files → Sends to FastAPI (as text or file)\n",
    "\n",
    " 3. FastAPI endpoint receives, orchestrates CrewAI agents, returns structured results\n",
    "\n",
    " 4. Streamlit displays results, progress, suggestions, etc.\n",
    "\n",
    "#### **Constraints**:\n",
    "\n",
    " - None\n",
    "\n",
    "\n",
    "#### **Tools**:\n",
    "\n",
    " - Use local **ollama** model\n",
    "\n",
    "#### **Requirements**:\n",
    " - Make it work as expected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786816a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5260bc82",
   "metadata": {},
   "source": [
    "## <center>**`Implementation`**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31c94b",
   "metadata": {},
   "source": [
    "## **`Backend`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb113ef",
   "metadata": {},
   "source": [
    "### Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999deab",
   "metadata": {},
   "source": [
    "#### DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a6a832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/storage/db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/storage/db.py\n",
    "# backend/app/storage/db.py\n",
    "\n",
    "import os\n",
    "from contextlib import contextmanager\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from backend.app.storage.models import Base\n",
    "\n",
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\n",
    "DB_PATH = os.path.join(DATA_DIR, \"app.sqlite3\")\n",
    "ENGINE = create_engine(f\"sqlite:///{DB_PATH}\", connect_args={\"check_same_thread\": False})\n",
    "SessionLocal = sessionmaker(bind=ENGINE, autoflush=False, autocommit=False)\n",
    "\n",
    "def init_db():\n",
    "    # For tests/dev, ensure tables exist. In prod, prefer `alembic upgrade head`.\n",
    "    if os.environ.get(\"DB_BOOTSTRAP\", \"orm\") == \"orm\":\n",
    "        Base.metadata.create_all(bind=ENGINE)\n",
    "\n",
    "def ensure_dirs():\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    os.makedirs(os.path.join(DATA_DIR, \"artifacts\"), exist_ok=True)\n",
    "    init_db()\n",
    "\n",
    "@contextmanager\n",
    "def get_session():\n",
    "    s = SessionLocal()\n",
    "    try:\n",
    "        yield s\n",
    "    finally:\n",
    "        s.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e18177",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90a2535a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/storage/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/storage/models.py\n",
    "# backend/app/storage/models.py\n",
    "from __future__ import annotations\n",
    "from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column\n",
    "from sqlalchemy import String, Enum, JSON, Text, Integer, DateTime, Index\n",
    "import enum\n",
    "import uuid\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def utcnow() -> datetime:\n",
    "    return datetime.now(timezone.utc)\n",
    "\n",
    "class Base(DeclarativeBase):\n",
    "    pass\n",
    "\n",
    "class RunStatus(str, enum.Enum):\n",
    "    queued = \"queued\"\n",
    "    running = \"running\"\n",
    "    succeeded = \"succeeded\"\n",
    "    failed = \"failed\"\n",
    "\n",
    "class Run(Base):\n",
    "    __tablename__ = \"runs\"\n",
    "\n",
    "    id: Mapped[str] = mapped_column(String, primary_key=True, default=lambda: str(uuid.uuid4()))\n",
    "    payload_hash: Mapped[str] = mapped_column(String, index=True)\n",
    "    status: Mapped[RunStatus] = mapped_column(Enum(RunStatus), index=True, default=RunStatus.queued)\n",
    "    error: Mapped[str | None] = mapped_column(Text, nullable=True)\n",
    "\n",
    "    # normalized inputs\n",
    "    resume_text: Mapped[str] = mapped_column(Text)\n",
    "    jd_text: Mapped[str] = mapped_column(Text)\n",
    "    params: Mapped[dict] = mapped_column(JSON, default=dict)\n",
    "\n",
    "    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=utcnow, nullable=False)\n",
    "    started_at: Mapped[datetime | None] = mapped_column(DateTime(timezone=True), nullable=True)\n",
    "    finished_at: Mapped[datetime | None] = mapped_column(DateTime(timezone=True), nullable=True)\n",
    "\n",
    "    __table_args__ = (\n",
    "        Index(\"ix_runs_payload_hash_status\", \"payload_hash\", \"status\"),\n",
    "    )\n",
    "\n",
    "class Artifact(Base):\n",
    "    __tablename__ = \"artifacts\"\n",
    "\n",
    "    id: Mapped[str] = mapped_column(String, primary_key=True, default=lambda: str(uuid.uuid4()))\n",
    "    run_id: Mapped[str] = mapped_column(String, index=True)\n",
    "    name: Mapped[str] = mapped_column(String)   # filename only\n",
    "    kind: Mapped[str] = mapped_column(String)   # e.g., \"scorecard\", \"trace\"\n",
    "    mime: Mapped[str] = mapped_column(String)   # e.g., \"application/json\"\n",
    "    path: Mapped[str] = mapped_column(String)   # absolute or relative path on disk\n",
    "\n",
    "    size_bytes: Mapped[int] = mapped_column(Integer, nullable=False, default=0)\n",
    "    sha256: Mapped[str] = mapped_column(String, index=True)\n",
    "\n",
    "    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=utcnow, nullable=False)\n",
    "\n",
    "    __table_args__ = (\n",
    "        Index(\"ix_artifacts_run_name\", \"run_id\", \"name\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4f871",
   "metadata": {},
   "source": [
    "#### Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad3b4601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/storage/artifacts.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/storage/artifacts.py\n",
    "# backend/app/storage/artifacts.py\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import hashlib\n",
    "import tempfile\n",
    "from typing import List\n",
    "from sqlalchemy.orm import Session\n",
    "from .models import Artifact\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\n",
    "\n",
    "MIME_BY_NAME = {\n",
    "    \"scorecard.json\": \"application/json\",\n",
    "    \"scorecard.md\": \"text/markdown\",\n",
    "    \"graph_trace.jsonl\": \"application/json\",\n",
    "    \"gaps.csv\": \"text/csv\",\n",
    "    # add more as needed\n",
    "}\n",
    "\n",
    "def run_dir(run_id: str) -> str:\n",
    "    base = os.path.join(DATA_DIR, \"artifacts\")\n",
    "    path = os.path.join(base, run_id)\n",
    "    # Ensure no path traversal\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def _safe_name(name: str) -> str:\n",
    "    # Reject path separators and traversal\n",
    "    bn = os.path.basename(name)\n",
    "    if bn != name or \"..\" in name or \"/\" in name or \"\\\\\" in name:\n",
    "        raise ValueError(\"invalid artifact name\")\n",
    "    return bn\n",
    "\n",
    "def _sha256_bytes(data: bytes) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    h.update(data)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _to_bytes(content: str | dict) -> bytes:\n",
    "    if isinstance(content, dict):\n",
    "        return json.dumps(content, ensure_ascii=False, indent=2).encode(\"utf-8\")\n",
    "    return content.encode(\"utf-8\")\n",
    "\n",
    "def _atomic_write(target_path: str, data: bytes) -> None:\n",
    "    # Write to temp file then rename to target (atomic on POSIX)\n",
    "    d = os.path.dirname(target_path)\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    fd, tmp_path = tempfile.mkstemp(prefix=\".tmp_\", dir=d)\n",
    "    try:\n",
    "        with os.fdopen(fd, \"wb\") as f:\n",
    "            f.write(data)\n",
    "            f.flush()\n",
    "            os.fsync(f.fileno())\n",
    "        os.replace(tmp_path, target_path)\n",
    "    finally:\n",
    "        try:\n",
    "            if os.path.exists(tmp_path):\n",
    "                os.remove(tmp_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def write_artifact(\n",
    "    s: Session,\n",
    "    run_id: str,\n",
    "    name: str,\n",
    "    kind: str,\n",
    "    mime: str | None,\n",
    "    content: str | dict,\n",
    ") -> Artifact:\n",
    "    name = _safe_name(name)\n",
    "    rd = run_dir(run_id)\n",
    "    fpath = os.path.join(rd, name)\n",
    "    data = _to_bytes(content)\n",
    "\n",
    "    # write atomically\n",
    "    _atomic_write(fpath, data)\n",
    "\n",
    "    size = len(data)\n",
    "    digest = _sha256_bytes(data)\n",
    "    mime = mime or MIME_BY_NAME.get(name, \"application/octet-stream\")\n",
    "\n",
    "    a = Artifact(\n",
    "        run_id=run_id,\n",
    "        name=name,\n",
    "        kind=kind,\n",
    "        mime=mime,\n",
    "        path=fpath,\n",
    "        size_bytes=size,\n",
    "        sha256=digest,\n",
    "        created_at=datetime.now(timezone.utc),\n",
    "    )\n",
    "    s.add(a)\n",
    "    s.commit()\n",
    "    return a\n",
    "\n",
    "def list_artifacts_for_run(s: Session, run_id: str) -> List[Artifact]:\n",
    "    # sorted by created_at then name for stability\n",
    "    return (\n",
    "        s.query(Artifact)\n",
    "        .filter(Artifact.run_id == run_id)\n",
    "        .order_by(Artifact.created_at.asc(), Artifact.name.asc())\n",
    "        .all()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b903c",
   "metadata": {},
   "source": [
    "### Migrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98f2c514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../migrations/env.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../migrations/env.py\n",
    "# migrations/env.py\n",
    "from __future__ import annotations\n",
    "from logging.config import fileConfig\n",
    "from sqlalchemy import engine_from_config, pool\n",
    "from alembic import context\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Ensure backend package is importable\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "\n",
    "from backend.app.storage.models import Base  # noqa: E402\n",
    "\n",
    "config = context.config\n",
    "if config.config_file_name is not None:\n",
    "    fileConfig(config.config_file_name)\n",
    "\n",
    "target_metadata = Base.metadata\n",
    "\n",
    "def run_migrations_offline():\n",
    "    url = config.get_main_option(\"sqlalchemy.url\")\n",
    "    context.configure(\n",
    "        url=url,\n",
    "        target_metadata=target_metadata,\n",
    "        literal_binds=True,\n",
    "        dialect_opts={\"paramstyle\": \"named\"},\n",
    "    )\n",
    "    with context.begin_transaction():\n",
    "        context.run_migrations()\n",
    "\n",
    "def run_migrations_online():\n",
    "    connectable = engine_from_config(\n",
    "        config.get_section(config.config_ini_section),\n",
    "        prefix=\"sqlalchemy.\",\n",
    "        poolclass=pool.NullPool,\n",
    "    )\n",
    "    with connectable.connect() as connection:\n",
    "        context.configure(connection=connection, target_metadata=target_metadata)\n",
    "        with context.begin_transaction():\n",
    "            context.run_migrations()\n",
    "\n",
    "if context.is_offline_mode():\n",
    "    run_migrations_offline()\n",
    "else:\n",
    "    run_migrations_online()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52722ba",
   "metadata": {},
   "source": [
    "#### Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4891d50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../migrations/versions/0001_baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../migrations/versions/0001_baseline.py\n",
    "# migrations/versions/0001_baseline.py\n",
    "\"\"\"baseline schema\n",
    "\n",
    "Revision ID: 0001_baseline\n",
    "Revises:\n",
    "Create Date: 2025-10-06 00:00:00\n",
    "\"\"\"\n",
    "from alembic import op\n",
    "import sqlalchemy as sa\n",
    "\n",
    "revision = \"0001_baseline\"\n",
    "down_revision = None\n",
    "branch_labels = None\n",
    "depends_on = None\n",
    "\n",
    "def upgrade() -> None:\n",
    "    op.create_table(\n",
    "        \"runs\",\n",
    "        sa.Column(\"id\", sa.String(), primary_key=True),\n",
    "        sa.Column(\"payload_hash\", sa.String(), index=True),\n",
    "        sa.Column(\"status\", sa.Enum(\"queued\", \"running\", \"succeeded\", \"failed\", name=\"runstatus\"), index=True),\n",
    "        sa.Column(\"error\", sa.Text(), nullable=True),\n",
    "        sa.Column(\"resume_text\", sa.Text()),\n",
    "        sa.Column(\"jd_text\", sa.Text()),\n",
    "        sa.Column(\"params\", sa.JSON()),\n",
    "        sa.Column(\"created_at\", sa.DateTime(timezone=True), nullable=False),\n",
    "        sa.Column(\"started_at\", sa.DateTime(timezone=True), nullable=True),\n",
    "        sa.Column(\"finished_at\", sa.DateTime(timezone=True), nullable=True),\n",
    "    )\n",
    "    op.create_index(\"ix_runs_payload_hash_status\", \"runs\", [\"payload_hash\", \"status\"])\n",
    "\n",
    "    op.create_table(\n",
    "        \"artifacts\",\n",
    "        sa.Column(\"id\", sa.String(), primary_key=True),\n",
    "        sa.Column(\"run_id\", sa.String(), index=True),\n",
    "        sa.Column(\"name\", sa.String()),\n",
    "        sa.Column(\"kind\", sa.String()),\n",
    "        sa.Column(\"mime\", sa.String()),\n",
    "        sa.Column(\"path\", sa.String()),\n",
    "        sa.Column(\"size_bytes\", sa.Integer(), nullable=False, server_default=\"0\"),\n",
    "        sa.Column(\"sha256\", sa.String(), index=True),\n",
    "        sa.Column(\"created_at\", sa.DateTime(timezone=True), nullable=False),\n",
    "    )\n",
    "    op.create_index(\"ix_artifacts_run_name\", \"artifacts\", [\"run_id\", \"name\"])\n",
    "\n",
    "def downgrade() -> None:\n",
    "    op.drop_index(\"ix_artifacts_run_name\", table_name=\"artifacts\")\n",
    "    op.drop_table(\"artifacts\")\n",
    "    op.drop_index(\"ix_runs_payload_hash_status\", table_name=\"runs\")\n",
    "    op.drop_table(\"runs\")\n",
    "    op.execute(\"DROP TYPE IF EXISTS runstatus\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec571e29",
   "metadata": {},
   "source": [
    "### Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd656cf",
   "metadata": {},
   "source": [
    "#### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "499fba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/core/graph.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/core/graph.py\n",
    "# backend/app/core/graph.py\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class GraphState:\n",
    "    resume_text: str\n",
    "    jd_text: str\n",
    "    skills_resume: List[str] = field(default_factory=list)\n",
    "    skills_jd: List[str] = field(default_factory=list)\n",
    "    coverage: float = 0.0\n",
    "    scorecard: Dict[str, Any] = field(default_factory=dict)\n",
    "    logs: List[Dict[str, Any]] = field(default_factory=list)\n",
    "\n",
    "SKILL_REGEX = re.compile(r\"\\b([A-Za-z][A-Za-z0-9\\+\\#\\.]{1,30})\\b\")\n",
    "\n",
    "COMMON_STOP = {\n",
    "    \"and\",\"or\",\"the\",\"a\",\"an\",\"with\",\"for\",\"to\",\"of\",\"in\",\"on\",\"at\",\"is\",\"are\",\"be\",\n",
    "    \"experience\",\"team\",\"work\",\"project\",\"projects\",\"skills\",\"tool\",\"tools\",\"stack\",\n",
    "    \"senior\",\"lead\",\"junior\",\"engineer\",\"developer\",\"manager\",\"product\",\"data\",\n",
    "}\n",
    "\n",
    "ALIAS = {\n",
    "    \"py\": \"python\",\n",
    "    \"js\": \"javascript\",\n",
    "    \"ts\": \"typescript\",\n",
    "    \"node\": \"nodejs\",\n",
    "    \"postgres\": \"postgresql\",\n",
    "    \"xgboost\": \"xgboost\",\n",
    "    \"ml\": \"machinelearning\",\n",
    "    \"dl\": \"deep learning\",\n",
    "    \"llm\": \"large language model\",\n",
    "    \"ai\": \"ai\",\n",
    "}\n",
    "\n",
    "def _norm_text(t: str) -> str:\n",
    "    t = t.replace(\"\\r\\n\", \"\\n\")\n",
    "    t = re.sub(r\"[\\x00-\\x08\\x0B-\\x1F\\x7F]\", \" \", t)\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "def _extract_skills(text: str) -> List[str]:\n",
    "    raw = [m.group(1).lower() for m in SKILL_REGEX.finditer(text)]\n",
    "    mapped = [ALIAS.get(x, x) for x in raw]\n",
    "    dedup = []\n",
    "    for x in mapped:\n",
    "        if x in COMMON_STOP:\n",
    "            continue\n",
    "        if x.isdigit():\n",
    "            continue\n",
    "        if len(x) < 2:\n",
    "            continue\n",
    "        if x not in dedup:\n",
    "            dedup.append(x)\n",
    "    return dedup[:128]  # cap for deterministic behavior\n",
    "\n",
    "def _coverage(a: List[str], b: List[str]) -> float:\n",
    "    if not b:\n",
    "        return 0.0\n",
    "    return round(100.0 * len(set(a) & set(b)) / len(set(b)), 1)\n",
    "\n",
    "def node_normalize(state: GraphState) -> GraphState:\n",
    "    state.resume_text = _norm_text(state.resume_text)\n",
    "    state.jd_text = _norm_text(state.jd_text)\n",
    "    state.logs.append({\"node\":\"normalize_text\", \"ok\": True})\n",
    "    return state\n",
    "\n",
    "def node_extract_skills(state: GraphState) -> GraphState:\n",
    "    state.skills_resume = _extract_skills(state.resume_text)\n",
    "    state.skills_jd = _extract_skills(state.jd_text)\n",
    "    state.logs.append({\n",
    "        \"node\": \"extract_skills_rule_based\",\n",
    "        \"resume_count\": len(state.skills_resume),\n",
    "        \"jd_count\": len(state.skills_jd),\n",
    "    })\n",
    "    return state\n",
    "\n",
    "def node_score_rule_based(state: GraphState) -> GraphState:\n",
    "    cov = _coverage(state.skills_resume, state.skills_jd)\n",
    "    # very simple dimensions\n",
    "    dims = {\n",
    "        \"skills_match\": cov,\n",
    "        \"keyword_density\": min(100.0, round(len(state.skills_resume)/3, 1)),\n",
    "        \"ats_hygiene\": 80.0,  # placeholder constant\n",
    "    }\n",
    "    overall = round(0.6 * dims[\"skills_match\"] + 0.25 * dims[\"keyword_density\"] + 0.15 * dims[\"ats_hygiene\"], 1)\n",
    "    state.coverage = cov\n",
    "    state.scorecard = {\n",
    "        \"overall_score\": overall,\n",
    "        \"dimensions\": dims,\n",
    "        \"coverage_terms_overlap\": sorted(list(set(state.skills_resume) & set(state.skills_jd)))[:25],\n",
    "    }\n",
    "    state.logs.append({\"node\": \"score_rule_based\", \"coverage\": cov, \"overall\": overall})\n",
    "    return state\n",
    "\n",
    "def node_build_scorecard(state: GraphState) -> GraphState:\n",
    "    # no-op here, but good place to format artifacts later\n",
    "    state.logs.append({\"node\": \"build_scorecard\", \"ok\": True})\n",
    "    return state\n",
    "\n",
    "def run_minimal_graph(resume_text: str, jd_text: str) -> GraphState:\n",
    "    state = GraphState(resume_text=resume_text, jd_text=jd_text)\n",
    "    for step in (node_normalize, node_extract_skills, node_score_rule_based, node_build_scorecard):\n",
    "        state = step(state)\n",
    "    return state\n",
    "\n",
    "def scorecard_markdown(state: GraphState) -> str:\n",
    "    sc = state.scorecard\n",
    "    dims = sc.get(\"dimensions\", {})\n",
    "    overlap = sc.get(\"coverage_terms_overlap\", [])\n",
    "    md = [\n",
    "        \"# Scorecard\",\n",
    "        f\"**Overall**: {sc.get('overall_score', 0)}/100\",\n",
    "        \"\",\n",
    "        \"## Dimensions\",\n",
    "        f\"- Skills Match: {dims.get('skills_match', 0)}\",\n",
    "        f\"- Keyword Density: {dims.get('keyword_density', 0)}\",\n",
    "        f\"- ATS Hygiene: {dims.get('ats_hygiene', 0)}\",\n",
    "        \"\",\n",
    "        \"## Overlap Terms\",\n",
    "        (\", \".join(overlap) if overlap else \"_none_\"),\n",
    "    ]\n",
    "    return \"\\n\".join(md)\n",
    "\n",
    "def trace_jsonl(state: GraphState) -> str:\n",
    "    return \"\\n\".join(json.dumps(e, ensure_ascii=False) for e in state.logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed6ed8",
   "metadata": {},
   "source": [
    "#### Run Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c57e7274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/core/run_manager.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/core/run_manager.py\n",
    "# backend/app/core/run_manager.py\n",
    "\n",
    "from __future__ import annotations\n",
    "from sqlalchemy.orm import Session\n",
    "from backend.app.storage.models import Run, RunStatus\n",
    "from backend.app.storage.artifacts import write_artifact\n",
    "import datetime\n",
    "from backend.app.core.graph import run_minimal_graph, scorecard_markdown, trace_jsonl\n",
    "\n",
    "class RunManager:\n",
    "    def __init__(self, s: Session, run: Run):\n",
    "        self.s = s\n",
    "        self.run = run\n",
    "\n",
    "    def _update_status(self, status: RunStatus, error: str | None = None):\n",
    "        self.run.status = status\n",
    "        self.run.error = error\n",
    "        if status == RunStatus.running:\n",
    "            self.run.started_at = datetime.datetime.now(datetime.UTC)\n",
    "        if status in (RunStatus.succeeded, RunStatus.failed):\n",
    "            self.run.finished_at = datetime.datetime.now(datetime.UTC)\n",
    "        self.s.add(self.run)\n",
    "        self.s.commit()\n",
    "\n",
    "    def execute(self):\n",
    "        self._update_status(RunStatus.running)\n",
    "\n",
    "        state = run_minimal_graph(self.run.resume_text, self.run.jd_text)\n",
    "\n",
    "        write_artifact(self.s, self.run.id, \"scorecard.json\", \"scorecard\", \"application/json\", state.scorecard)\n",
    "        write_artifact(self.s, self.run.id, \"scorecard.md\", \"scorecard\", \"text/markdown\", scorecard_markdown(state))\n",
    "        write_artifact(self.s, self.run.id, \"graph_trace.jsonl\", \"trace\", \"application/json\", trace_jsonl(state))\n",
    "\n",
    "        self._update_status(RunStatus.succeeded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ed902",
   "metadata": {},
   "source": [
    "#### Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc66b9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/core/queue.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/core/queue.py\n",
    "# backend/app/core/queue.py\n",
    "import os\n",
    "import asyncio\n",
    "from arq import create_pool\n",
    "from arq.connections import RedisSettings\n",
    "from sqlalchemy.orm import Session\n",
    "from backend.app.storage.db import get_session, ensure_dirs\n",
    "from backend.app.storage.models import Run, RunStatus\n",
    "from backend.app.core.run_manager import RunManager\n",
    "\n",
    "REDIS_URL = os.environ.get(\"REDIS_URL\", \"redis://host.docker.internal:6379/0\")\n",
    "\n",
    "async def enqueue_run(run_id: str):\n",
    "    redis = await create_pool(RedisSettings.from_dsn(REDIS_URL))\n",
    "    await redis.enqueue_job(\"run_match_job\", run_id=run_id)\n",
    "\n",
    "async def run_match_job(ctx, run_id: str):\n",
    "    ensure_dirs()\n",
    "    # Do all heavy work here (worker process with its own loop)\n",
    "    with get_session() as s:\n",
    "        run = s.get(Run, run_id)\n",
    "        if not run:\n",
    "            return\n",
    "        if run.status not in (RunStatus.queued, RunStatus.failed):\n",
    "            return\n",
    "        mgr = RunManager(s, run)\n",
    "        try:\n",
    "            mgr.execute()\n",
    "        except Exception as e:\n",
    "            run.status = RunStatus.failed\n",
    "            run.error = str(e)\n",
    "            s.add(run)\n",
    "            s.commit()\n",
    "\n",
    "class WorkerSettings:\n",
    "    redis_settings = RedisSettings.from_dsn(REDIS_URL)\n",
    "    functions = [run_match_job]\n",
    "    max_jobs = 10\n",
    "    retry_jobs = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b142dc1",
   "metadata": {},
   "source": [
    "### Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be9245ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/worker.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/worker.py\n",
    "# backend/worker.py\n",
    "\n",
    "# Convenient entrypoint to run worker without module path issues\n",
    "from typing import cast\n",
    "from arq import run_worker\n",
    "from arq.typing import WorkerSettingsType\n",
    "from backend.app.core.queue import WorkerSettings\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_worker(cast(WorkerSettingsType, WorkerSettings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee7a877",
   "metadata": {},
   "source": [
    "### Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "112765ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/models/schemas.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/models/schemas.py\n",
    "# backend/app/models/schemas.py\n",
    "from typing import Optional, Dict, List, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class RunRequest(BaseModel):\n",
    "    resume_text: str = Field(..., min_length=1, description=\"Plain text resume\")\n",
    "    jd_text: str = Field(..., min_length=1, description=\"Plain text job description\")\n",
    "    params: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class RunResponse(BaseModel):\n",
    "    run_id: str\n",
    "    status: str\n",
    "\n",
    "class ArtifactMeta(BaseModel):\n",
    "    name: str\n",
    "    kind: str\n",
    "    mime: str\n",
    "    size_bytes: Optional[int] = None\n",
    "\n",
    "class RunStatusResponse(BaseModel):\n",
    "    run_id: str\n",
    "    status: str\n",
    "    error: Optional[str] = None\n",
    "    artifacts: List[ArtifactMeta] = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e120151",
   "metadata": {},
   "source": [
    "### API Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de0c5d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/api/routes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/api/routes.py\n",
    "# backend/app/api/routes.py\n",
    "import json\n",
    "import hashlib\n",
    "from typing import Optional\n",
    "\n",
    "from fastapi import APIRouter, HTTPException\n",
    "from sqlalchemy import select\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from backend.app.models.schemas import RunRequest, RunResponse, RunStatusResponse, ArtifactMeta\n",
    "from backend.app.storage.db import get_session, ensure_dirs\n",
    "from backend.app.storage.models import Run, RunStatus\n",
    "from backend.app.core.queue import enqueue_run\n",
    "from backend.app.storage.artifacts import list_artifacts_for_run\n",
    "from fastapi.responses import FileResponse\n",
    "from backend.app.storage.artifacts import run_dir\n",
    "\n",
    "\n",
    "router = APIRouter()\n",
    "\n",
    "MAX_LEN = 2_000_000  # ~2MB chars\n",
    "\n",
    "\n",
    "@router.post(\"/runs\", response_model=RunResponse, status_code=202)\n",
    "async def create_run(req: RunRequest) -> RunResponse:\n",
    "    \"\"\"Queue a new run. FastAPI will validate the body into RunRequest automatically.\"\"\"\n",
    "\n",
    "    if len(req.resume_text) > MAX_LEN or len(req.jd_text) > MAX_LEN:\n",
    "        raise HTTPException(status_code=413, detail=\"payload too large\")\n",
    "\n",
    "    ensure_dirs()\n",
    "\n",
    "    # Simple idempotency hash\n",
    "    h = hashlib.sha256(\n",
    "        (\n",
    "            req.resume_text.strip()\n",
    "            + \"\\n---\\n\"\n",
    "            + req.jd_text.strip()\n",
    "            + json.dumps(req.params or {}, sort_keys=True)\n",
    "        ).encode(\"utf-8\")\n",
    "    ).hexdigest()\n",
    "\n",
    "    with get_session() as s:\n",
    "        existing = (\n",
    "            s.execute(\n",
    "                select(Run)\n",
    "                .where(Run.payload_hash == h, Run.status == RunStatus.succeeded)\n",
    "                .order_by(Run.finished_at.desc())   # take newest\n",
    "            )\n",
    "            .scalars()\n",
    "            .first()\n",
    "        )\n",
    "        if existing:\n",
    "            return RunResponse(run_id=existing.id, status=existing.status.value)\n",
    "\n",
    "        run = Run(\n",
    "            payload_hash=h,\n",
    "            status=RunStatus.queued,\n",
    "            resume_text=req.resume_text,\n",
    "            jd_text=req.jd_text,\n",
    "            params=req.params or {},\n",
    "        )\n",
    "        s.add(run)\n",
    "        s.commit()\n",
    "        s.refresh(run)\n",
    "\n",
    "    # enqueue the job\n",
    "    await enqueue_run(run_id=run.id)\n",
    "\n",
    "    return RunResponse(run_id=run.id, status=str(RunStatus.queued.value))\n",
    "\n",
    "\n",
    "@router.get(\"/runs/{run_id}\", response_model=RunStatusResponse)\n",
    "async def get_run(run_id: str) -> RunStatusResponse:\n",
    "    with get_session() as s:\n",
    "        run = s.get(Run, run_id)\n",
    "        if not run:\n",
    "            raise HTTPException(status_code=404, detail=\"run not found\")\n",
    "\n",
    "        artifacts = []\n",
    "        if run.status == RunStatus.succeeded:\n",
    "            artifacts = [\n",
    "                ArtifactMeta(name=a.name, kind=a.kind, mime=a.mime, size_bytes=a.size_bytes)\n",
    "                for a in list_artifacts_for_run(s, run_id)\n",
    "            ]\n",
    "\n",
    "        return RunStatusResponse(\n",
    "            run_id=run.id, status=str(run.status.value), error=run.error, artifacts=artifacts\n",
    "        )\n",
    "\n",
    "\n",
    "@router.get(\"/artifacts/{run_id}\")\n",
    "async def list_artifacts(run_id: str):\n",
    "    with get_session() as s:\n",
    "        run = s.get(Run, run_id)\n",
    "        if not run:\n",
    "            raise HTTPException(status_code=404, detail=\"run not found\")\n",
    "        arts = list_artifacts_for_run(s, run_id)\n",
    "        files = [\n",
    "            {\"name\": a.name, \"kind\": a.kind, \"mime\": a.mime, \"size_bytes\": a.size_bytes}\n",
    "            for a in arts\n",
    "        ]\n",
    "        return {\"run_id\": run_id, \"files\": files}\n",
    "\n",
    "\n",
    "@router.get(\"/artifacts/{run_id}/{name}\")\n",
    "async def get_artifact(run_id: str, name: str):\n",
    "    with get_session() as s:\n",
    "        run = s.get(Run, run_id)\n",
    "        if not run:\n",
    "            raise HTTPException(status_code=404, detail=\"run not found\")\n",
    "        arts = list_artifacts_for_run(s, run_id)\n",
    "        for a in arts:\n",
    "            if a.name == name:\n",
    "                # Serve with content-disposition for nice filename in downloads\n",
    "                return FileResponse(\n",
    "                    a.path,\n",
    "                    media_type=a.mime,\n",
    "                    filename=a.name,\n",
    "                )\n",
    "    raise HTTPException(status_code=404, detail=\"artifact not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f6a465",
   "metadata": {},
   "source": [
    "### Main app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3bee2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/main.py\n",
    "# backend/app/main.py\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from backend.app.api.routes import router\n",
    "\n",
    "app = FastAPI(title=\"JobMatch-AI API\", version=\"0.1.0\")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"]\n",
    ")\n",
    "\n",
    "@app.get(\"/healthz\")\n",
    "async def healtz():\n",
    "    return {\"ok\": True}\n",
    "\n",
    "app.include_router(router, prefix=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829f36a",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674042b9",
   "metadata": {},
   "source": [
    "#### ConfTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bb572b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../tests/conftest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../tests/conftest.py\n",
    "# tests/conftest.py\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import pytest\n",
    "\n",
    "# Set DATA_DIR & ENV *before* importing any backend code\n",
    "TEST_DATA_DIR = tempfile.mkdtemp(prefix=\"jobmatch_test_data_\")\n",
    "os.environ[\"DATA_DIR\"] = TEST_DATA_DIR\n",
    "os.environ[\"ENV\"] = \"test\"\n",
    "\n",
    "# Add repo root to PYTHONPATH for tests\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "# Force AnyIO to use asyncio to avoid trio dependency\n",
    "@pytest.fixture\n",
    "def anyio_backend():\n",
    "    return \"asyncio\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc1f1de",
   "metadata": {},
   "source": [
    "#### Api Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c8af68",
   "metadata": {},
   "source": [
    "##### W/O Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17b153c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../tests/test_api_contract.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../tests/test_api_contract.py\n",
    "# tests/test_api_contract.py\n",
    "import pytest\n",
    "import uuid\n",
    "from httpx import AsyncClient, ASGITransport\n",
    "from backend.app.main import app\n",
    "from backend.app.storage.db import ensure_dirs\n",
    "\n",
    "\n",
    "@pytest.mark.anyio\n",
    "async def test_healthz():\n",
    "    transport = ASGITransport(app=app)\n",
    "    async with AsyncClient(transport=transport, base_url=\"http://test\") as ac:\n",
    "        r = await ac.get(\"/healthz\")\n",
    "        assert r.status_code == 200\n",
    "        assert r.json()[\"ok\"] is True\n",
    "\n",
    "\n",
    "@pytest.mark.anyio\n",
    "async def test_create_and_poll_run(monkeypatch):\n",
    "    ensure_dirs()\n",
    "\n",
    "    async def _noop_enqueue_run(*args, **kwargs):\n",
    "        return None\n",
    "\n",
    "    import backend.app.api.routes as routes_mod\n",
    "    monkeypatch.setattr(routes_mod, \"enqueue_run\", _noop_enqueue_run)\n",
    "\n",
    "    transport = ASGITransport(app=app)\n",
    "    async with AsyncClient(transport=transport, base_url=\"http://test\") as ac:\n",
    "        payload = {\n",
    "            \"resume_text\": f\"A B C {uuid.uuid4()}\",\n",
    "            \"jd_text\": \"A B\",\n",
    "            \"params\": {}\n",
    "        }\n",
    "        r = await ac.post(\"/runs\", json=payload)\n",
    "        assert r.status_code in (200, 202)\n",
    "        run_id = r.json()[\"run_id\"]\n",
    "\n",
    "        r2 = await ac.get(f\"/runs/{run_id}\")\n",
    "        assert r2.status_code == 200\n",
    "        body = r2.json()\n",
    "        assert body[\"run_id\"] == run_id\n",
    "        assert body[\"status\"] in (\"queued\", \"running\", \"succeeded\", \"failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73b128",
   "metadata": {},
   "source": [
    "##### With Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afc88ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../tests/test_api_contract.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../tests/test_api_contract.py\n",
    "# tests/test_api_contract.py\n",
    "import pytest\n",
    "from httpx import AsyncClient\n",
    "from httpx import ASGITransport \n",
    "from backend.app.main import app\n",
    "from backend.app.storage.db import ensure_dirs\n",
    "\n",
    "@pytest.mark.anyio\n",
    "async def test_healthz():\n",
    "    transport = ASGITransport(app=app)  # NEW\n",
    "    async with AsyncClient(transport=transport, base_url=\"http://test\") as ac:\n",
    "        r = await ac.get(\"/healthz\")\n",
    "        assert r.status_code == 200\n",
    "        assert r.json()[\"ok\"] is True\n",
    "\n",
    "@pytest.mark.anyio\n",
    "async def test_create_and_poll_run(monkeypatch):\n",
    "    ensure_dirs()\n",
    "    transport = ASGITransport(app=app)  # NEW\n",
    "    async with AsyncClient(transport=transport, base_url=\"http://test\") as ac:\n",
    "        r = await ac.post(\"/runs\", json={\"resume_text\": \"A B C\", \"jd_text\": \"A B\", \"params\": {}})\n",
    "        assert r.status_code in (200, 202)\n",
    "        run_id = r.json()[\"run_id\"]\n",
    "\n",
    "        r2 = await ac.get(f\"/runs/{run_id}\")\n",
    "        assert r2.status_code == 200\n",
    "        body = r2.json()\n",
    "        assert body[\"run_id\"] == run_id\n",
    "        assert body[\"status\"] in (\"queued\", \"running\", \"succeeded\", \"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c8a8b",
   "metadata": {},
   "source": [
    "#### Test graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eeaa681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../tests/test_graph_unit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../tests/test_graph_unit.py\n",
    "# tests/test_graph_unit.py\n",
    "\n",
    "from backend.app.core.graph import run_minimal_graph\n",
    "\n",
    "def test_graph_basic():\n",
    "    resume = \"Built APIs in Python & FastAPI. Used PostgreSQL and Docker. Deployed on AWS.\"\n",
    "    jd = \"Looking for a Python developer with FastAPI, PostgreSQL, and AWS experience.\"\n",
    "    st = run_minimal_graph(resume, jd)\n",
    "    assert st.scorecard[\"overall_score\"] > 50\n",
    "    assert \"python\" in st.skills_resume\n",
    "    assert \"fastapi\" in st.skills_resume\n",
    "    assert st.coverage >= 50.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebae484f",
   "metadata": {},
   "source": [
    "## **`Frontend`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7d605",
   "metadata": {},
   "source": [
    "### Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edaa600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../frontend/streamlit_app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../frontend/streamlit_app/main.py\n",
    "# streamlit_app/main.py\n",
    "import os\n",
    "import asyncio\n",
    "import httpx\n",
    "import streamlit as st\n",
    "\n",
    "API_BASE = os.environ.get(\"API_BASE\", \"http://localhost:8000\")\n",
    "\n",
    "st.set_page_config(page_title=\"JobMatch-AI\", layout=\"wide\")\n",
    "st.title(\"JobMatch-AI — Resume ↔ JD Matcher\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.markdown(\"**Status**: Test (stub graph)\")\n",
    "    st.markdown(\"API: \" + API_BASE)\n",
    "\n",
    "resume_text = st.text_area(\"Paste your Resume (plain text)\", height=220)\n",
    "jd_text = st.text_area(\"Paste the Job Description (plain text)\", height=220)\n",
    "params_col1, params_col2 = st.columns(2)\n",
    "with params_col1:\n",
    "    threshold = st.slider(\"Minimum score threshold (UI only)\", 0, 100, 65)\n",
    "\n",
    "run_btn = st.button(\"Run Match\", type=\"primary\", use_container_width=True)\n",
    "\n",
    "status_placeholder = st.empty()\n",
    "artifacts_placeholder = st.empty()\n",
    "\n",
    "async def poll_status(run_id: str):\n",
    "    status_placeholder.info(f\"Run `{run_id}` queued…\")\n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        while True:\n",
    "            r = await client.get(f\"{API_BASE}/runs/{run_id}\")\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            if data[\"status\"] == \"succeeded\":\n",
    "                status_placeholder.success(f\"Run `{run_id}`: succeeded ✅\")\n",
    "                return data\n",
    "            if data[\"status\"] == \"failed\":\n",
    "                status_placeholder.error(f\"Run `{run_id}`: failed ❌ — {data.get('error')}\")\n",
    "                return data\n",
    "            status_placeholder.info(f\"Run `{run_id}`: {data['status']}…\")\n",
    "            await asyncio.sleep(1.0)\n",
    "\n",
    "if run_btn:\n",
    "    if not resume_text.strip() or not jd_text.strip():\n",
    "        st.warning(\"Please paste both Resume and Job Description.\")\n",
    "    else:\n",
    "        with st.spinner(\"Submitting…\"):\n",
    "            resp = httpx.post(f\"{API_BASE}/runs\", json={\"resume_text\": resume_text, \"jd_text\": jd_text, \"params\": {\"ui_threshold\": threshold}})\n",
    "            if resp.status_code not in (200, 202):\n",
    "                st.error(f\"Error: {resp.text}\")\n",
    "            else:\n",
    "                run_id = resp.json()[\"run_id\"]\n",
    "                data = asyncio.run(poll_status(run_id))\n",
    "                if data and data.get(\"artifacts\"):\n",
    "                    with artifacts_placeholder.container():\n",
    "                        st.subheader(\"Artifacts\")\n",
    "                        for a in data[\"artifacts\"]:\n",
    "                            url = f\"{API_BASE}/artifacts/{run_id}/{a['name']}\"\n",
    "                            st.write(f\"- **{a['name']}** ({a['mime']}) — [download]({url})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c718505b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-jd-matcher-langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
