{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b82be6f",
   "metadata": {},
   "source": [
    "# <center>**`Project Details`**</center>\n",
    "\n",
    "#### **Purpose**:\n",
    "\n",
    "Tis project goal is matching a resume to a job description. A poorly aligned resume can lead to missed opportunities, even when the candidate is a strong fit. The project aims to showcase how we can make use of AI agents to help applicants tailor their resumes more strategically, uncover hidden gaps, and present a stronger case to recruiters — all with minimal effort.\n",
    "\n",
    "A [Github repo](https://github.com/vikrambhat2/MultiAgents-with-CrewAI-ResumeJDMatcher) tackling this project exist and our goal will be to improve it by seperating the **Backend** and the **Frontend** logics.\n",
    "\n",
    "##### **Why Split**?\n",
    "\n",
    " - The backend will hold business logic, agent orchestration, model calls, state, data processing, API endpoints, while the frontend focuses on the UI/UX, user interaction, session management, file upload, displaying results.\n",
    " - Scalability: Backend can scale independently of UI (and can even serve other clients)\n",
    " - Security: Sensitive logic, API keys, and resource-intensive processing are kept server-side\n",
    " - Performance: Streamlit remains snappy, while heavy lifting is offloaded to backend\n",
    " \n",
    "##### **Responsibilities**\n",
    "\n",
    "  - *<u>Backend</u>*: \n",
    "    - Expose REST API endpoints:\n",
    "        - `/match`: Accepts resume + JD, returns match results and insights.\n",
    "        - `/enhance`: Accepts resume + JD, returns resume improvement suggestions.\n",
    "        - `/cover-letter`: Accepts resume + JD, returns a cover letter.\n",
    "    - Agent orchestration: All CrewAI workflows run here.\n",
    "    - Input validation, error handling.\n",
    "    - PDF/text parsing if desired (or can also be handled in frontend, see below).\n",
    "    - Optional: Authentication, user/session management, logging, monitoring.\n",
    "    - Optional: Serve as an async queue for heavy jobs if latency is an issue (using Celery/RQ, etc.).\n",
    " \n",
    " - *<u>Frontend</u>(Streamlit)*\n",
    "    - UI for uploading files, entering/pasting text.\n",
    "    - Visualization: Render reports, scores, enhanced resume, cover letter, etc.\n",
    "    - API client: Handles all interaction with FastAPI backend.\n",
    "    - Light preprocessing: E.g., local PDF parsing if you want to send plain text to backend (saves bandwidth).\n",
    "    - Session/user state, feedback, download links, etc.\n",
    "\n",
    "Here is how the system works (Flow):\n",
    "\n",
    " 1. User uploads resume & JD (PDF or text) → Streamlit UI\n",
    "\n",
    " 2. Frontend extracts or passes files → Sends to FastAPI (as text or file)\n",
    "\n",
    " 3. FastAPI endpoint receives, orchestrates CrewAI agents, returns structured results\n",
    "\n",
    " 4. Streamlit displays results, progress, suggestions, etc.\n",
    "\n",
    "#### **Constraints**:\n",
    "\n",
    " - None\n",
    "\n",
    "\n",
    "#### **Tools**:\n",
    "\n",
    " - Use local **ollama** model\n",
    "\n",
    "#### **Requirements**:\n",
    " - Make it work as expected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786816a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5260bc82",
   "metadata": {},
   "source": [
    "## <center>**`Implementation`**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31c94b",
   "metadata": {},
   "source": [
    "## **`Backend`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb113ef",
   "metadata": {},
   "source": [
    "### Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999deab",
   "metadata": {},
   "source": [
    "#### DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a6a832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/storage/db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/storage/db.py\n",
    "# backend/app/storage/db.py\n",
    "\n",
    "import os\n",
    "from contextlib import contextmanager\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from backend.app.storage.models import Base\n",
    "\n",
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\n",
    "DB_PATH = os.path.join(DATA_DIR, \"app.sqlite3\")\n",
    "ENGINE = create_engine(f\"sqlite:///{DB_PATH}\", connect_args={\"check_same_thread\":False})\n",
    "SessionLocal = sessionmaker(bind=ENGINE, autoflush=False, autocommit=False)\n",
    "\n",
    "def init_db():\n",
    "    Base.metadata.create_all(bind=ENGINE)\n",
    "\n",
    "def ensure_dirs():\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    os.makedirs(os.path.join(DATA_DIR, \"artifacts\"), exist_ok=True)\n",
    "    init_db()\n",
    "\n",
    "@contextmanager\n",
    "def get_session():\n",
    "    s = SessionLocal()\n",
    "    try:\n",
    "        yield s\n",
    "    finally:\n",
    "        s.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e18177",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90a2535a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/storage/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/storage/models.py\n",
    "# backend/app/storage/models.py\n",
    "from __future__ import annotations\n",
    "from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column\n",
    "from sqlalchemy import String, Enum, JSON, Text\n",
    "import enum\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "class Base(DeclarativeBase):\n",
    "    pass\n",
    "\n",
    "class RunStatus(str, enum.Enum):\n",
    "    queued = \"queued\"\n",
    "    running = \"running\"\n",
    "    succeeded = \"succeeded\"\n",
    "    failed = \"failed\"\n",
    "\n",
    "class Run(Base):\n",
    "    __tablename__ = \"runs\"\n",
    "    id: Mapped[str] = mapped_column(String, primary_key=True, default=lambda: str(uuid.uuid4()))\n",
    "    payload_hash: Mapped[str] = mapped_column(String, index=True)\n",
    "    status: Mapped[RunStatus] = mapped_column(Enum(RunStatus), default=RunStatus.queued)\n",
    "    error: Mapped[str | None] = mapped_column(Text, nullable=True)\n",
    "\n",
    "    # store normalized inputs (simple for M0-M3)\n",
    "    resume_text: Mapped[str] = mapped_column(Text)\n",
    "    jd_text: Mapped[str] = mapped_column(Text)\n",
    "    params: Mapped[dict] = mapped_column(JSON, default=dict)\n",
    "\n",
    "    started_at: Mapped[datetime | None] = mapped_column(nullable=True)\n",
    "    finished_at: Mapped[datetime | None] = mapped_column(nullable=True)\n",
    "\n",
    "class Artifact(Base):\n",
    "    __tablename__ = \"artifacts\"\n",
    "    id: Mapped[str] = mapped_column(String, primary_key=True, default=lambda: str(uuid.uuid4()))\n",
    "    run_id: Mapped[str] = mapped_column(String, index=True)\n",
    "    name: Mapped[str] = mapped_column(String)  # filename\n",
    "    kind: Mapped[str] = mapped_column(String)  # e.g., \"scorecard\", \"trace\"\n",
    "    mime: Mapped[str] = mapped_column(String)  # e.g., \"application/json\"\n",
    "    path: Mapped[str] = mapped_column(String)  # absolute or relative path on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4f871",
   "metadata": {},
   "source": [
    "#### Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad3b4601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../backend/app/storage/artifacts.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/storage/artifacts.py\n",
    "# backend/app/storage/artifacts.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from sqlalchemy.orm import Session\n",
    "from backend.app.storage.models import Artifact\n",
    "\n",
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\n",
    "\n",
    "def run_dir(run_id: str) -> str:\n",
    "    return os.path.join(DATA_DIR, \"artifacts\", run_id)\n",
    "\n",
    "def write_artifact(s: Session, run_id: str, name: str, kind: str, mime: str, content: str | dict):\n",
    "    os.makedirs(run_dir(run_id), exist_ok=True)\n",
    "    fpath = os.path.join(run_dir(run_id), name)\n",
    "    if isinstance(content, dict):\n",
    "        with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(content, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    else:\n",
    "        with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "    a = Artifact(run_id=run_id, name=name, kind=kind, mime=mime, path=fpath)\n",
    "    s.add(a)\n",
    "    s.commit()\n",
    "\n",
    "def list_artifacts_for_run(s: Session, run_id: str) -> List[Artifact]:\n",
    "    return s.query(Artifact).filter(Artifact.run_id == run_id).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec571e29",
   "metadata": {},
   "source": [
    "### Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd656cf",
   "metadata": {},
   "source": [
    "#### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "499fba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../backend/app/core/graph.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/core/graph.py\n",
    "# backend/app/core/graph.py\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class GraphState:\n",
    "    resume_text: str\n",
    "    jd_text: str\n",
    "    skills_resume: List[str] = field(default_factory=list)\n",
    "    skills_jd: List[str] = field(default_factory=list)\n",
    "    coverage: float = 0.0\n",
    "    scorecard: Dict[str, Any] = field(default_factory=dict)\n",
    "    logs: List[Dict[str, Any]] = field(default_factory=list)\n",
    "\n",
    "SKILL_REGEX = re.compile(r\"\\b([A-Za-z][A-Za-z0-9\\+\\#\\.]{1,30})\\b\")\n",
    "\n",
    "COMMON_STOP = {\n",
    "    \"and\",\"or\",\"the\",\"a\",\"an\",\"with\",\"for\",\"to\",\"of\",\"in\",\"on\",\"at\",\"is\",\"are\",\"be\",\n",
    "    \"experience\",\"team\",\"work\",\"project\",\"projects\",\"skills\",\"tool\",\"tools\",\"stack\",\n",
    "    \"senior\",\"lead\",\"junior\",\"engineer\",\"developer\",\"manager\",\"product\",\"data\",\n",
    "}\n",
    "\n",
    "ALIAS = {\n",
    "    \"py\": \"python\",\n",
    "    \"js\": \"javascript\",\n",
    "    \"ts\": \"typescript\",\n",
    "    \"node\": \"nodejs\",\n",
    "    \"postgres\": \"postgresql\",\n",
    "    \"xgboost\": \"xgboost\",\n",
    "    \"ml\": \"machinelearning\",\n",
    "    \"dl\": \"deep learning\",\n",
    "    \"llm\": \"large language model\",\n",
    "    \"ai\": \"ai\",\n",
    "}\n",
    "\n",
    "def _norm_text(t: str) -> str:\n",
    "    t = t.replace(\"\\r\\n\", \"\\n\")\n",
    "    t = re.sub(r\"[\\x00-\\x08\\x0B-\\x1F\\x7F]\", \" \", t)\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "def _extract_skills(text: str) -> List[str]:\n",
    "    raw = [m.group(1).lower() for m in SKILL_REGEX.finditer(text)]\n",
    "    mapped = [ALIAS.get(x, x) for x in raw]\n",
    "    dedup = []\n",
    "    for x in mapped:\n",
    "        if x in COMMON_STOP:\n",
    "            continue\n",
    "        if x.isdigit():\n",
    "            continue\n",
    "        if len(x) < 2:\n",
    "            continue\n",
    "        if x not in dedup:\n",
    "            dedup.append(x)\n",
    "    return dedup[:128]  # cap for deterministic behavior\n",
    "\n",
    "def _coverage(a: List[str], b: List[str]) -> float:\n",
    "    if not b:\n",
    "        return 0.0\n",
    "    return round(100.0 * len(set(a) & set(b)) / len(set(b)), 1)\n",
    "\n",
    "def node_normalize(state: GraphState) -> GraphState:\n",
    "    state.resume_text = _norm_text(state.resume_text)\n",
    "    state.jd_text = _norm_text(state.jd_text)\n",
    "    state.logs.append({\"node\":\"normalize_text\", \"ok\": True})\n",
    "    return state\n",
    "\n",
    "def node_extract_skills(state: GraphState) -> GraphState:\n",
    "    state.skills_resume = _extract_skills(state.resume_text)\n",
    "    state.skills_jd = _extract_skills(state.jd_text)\n",
    "    state.logs.append({\n",
    "        \"node\": \"extract_skills_rule_based\",\n",
    "        \"resume_count\": len(state.skills_resume),\n",
    "        \"jd_count\": len(state.skills_jd),\n",
    "    })\n",
    "    return state\n",
    "\n",
    "def node_score_rule_based(state: GraphState) -> GraphState:\n",
    "    cov = _coverage(state.skills_resume, state.skills_jd)\n",
    "    # very simple dimensions\n",
    "    dims = {\n",
    "        \"skills_match\": cov,\n",
    "        \"keyword_density\": min(100.0, round(len(state.skills_resume)/3, 1)),\n",
    "        \"ats_hygiene\": 80.0,  # placeholder constant\n",
    "    }\n",
    "    overall = round(0.6 * dims[\"skills_match\"] + 0.25 * dims[\"keyword_density\"] + 0.15 * dims[\"ats_hygiene\"], 1)\n",
    "    state.coverage = cov\n",
    "    state.scorecard = {\n",
    "        \"overall_score\": overall,\n",
    "        \"dimensions\": dims,\n",
    "        \"coverage_terms_overlap\": sorted(list(set(state.skills_resume) & set(state.skills_jd)))[:25],\n",
    "    }\n",
    "    state.logs.append({\"node\": \"score_rule_based\", \"coverage\": cov, \"overall\": overall})\n",
    "    return state\n",
    "\n",
    "def node_build_scorecard(state: GraphState) -> GraphState:\n",
    "    # no-op here, but good place to format artifacts later\n",
    "    state.logs.append({\"node\": \"build_scorecard\", \"ok\": True})\n",
    "    return state\n",
    "\n",
    "def run_minimal_graph(resume_text: str, jd_text: str) -> GraphState:\n",
    "    state = GraphState(resume_text=resume_text, jd_text=jd_text)\n",
    "    for step in (node_normalize, node_extract_skills, node_score_rule_based, node_build_scorecard):\n",
    "        state = step(state)\n",
    "    return state\n",
    "\n",
    "def scorecard_markdown(state: GraphState) -> str:\n",
    "    sc = state.scorecard\n",
    "    dims = sc.get(\"dimensions\", {})\n",
    "    overlap = sc.get(\"coverage_terms_overlap\", [])\n",
    "    md = [\n",
    "        \"# Scorecard\",\n",
    "        f\"**Overall**: {sc.get('overall_score', 0)}/100\",\n",
    "        \"\",\n",
    "        \"## Dimensions\",\n",
    "        f\"- Skills Match: {dims.get('skills_match', 0)}\",\n",
    "        f\"- Keyword Density: {dims.get('keyword_density', 0)}\",\n",
    "        f\"- ATS Hygiene: {dims.get('ats_hygiene', 0)}\",\n",
    "        \"\",\n",
    "        \"## Overlap Terms\",\n",
    "        (\", \".join(overlap) if overlap else \"_none_\"),\n",
    "    ]\n",
    "    return \"\\n\".join(md)\n",
    "\n",
    "def trace_jsonl(state: GraphState) -> str:\n",
    "    return \"\\n\".join(json.dumps(e, ensure_ascii=False) for e in state.logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed6ed8",
   "metadata": {},
   "source": [
    "#### Run Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c57e7274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/core/run_manager.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/core/run_manager.py\n",
    "# backend/app/core/run_manager.py\n",
    "\n",
    "from __future__ import annotations\n",
    "from sqlalchemy.orm import Session\n",
    "from backend.app.storage.models import Run, RunStatus\n",
    "from backend.app.storage.artifacts import write_artifact\n",
    "import datetime\n",
    "from backend.app.core.graph import run_minimal_graph, scorecard_markdown, trace_jsonl\n",
    "\n",
    "class RunManager:\n",
    "    def __init__(self, s: Session, run: Run):\n",
    "        self.s = s\n",
    "        self.run = run\n",
    "\n",
    "    def _update_status(self, status: RunStatus, error: str | None = None):\n",
    "        self.run.status = status\n",
    "        self.run.error = error\n",
    "        if status == RunStatus.running:\n",
    "            self.run.started_at = datetime.datetime.now(datetime.UTC)\n",
    "        if status in (RunStatus.succeeded, RunStatus.failed):\n",
    "            self.run.finished_at = datetime.datetime.now(datetime.UTC)\n",
    "        self.s.add(self.run)\n",
    "        self.s.commit()\n",
    "\n",
    "    def execute(self):\n",
    "        self._update_status(RunStatus.running)\n",
    "\n",
    "        state = run_minimal_graph(self.run.resume_text, self.run.jd_text)\n",
    "\n",
    "        write_artifact(self.s, self.run.id, \"scorecard.json\", \"scorecard\", \"application/json\", state.scorecard)\n",
    "        write_artifact(self.s, self.run.id, \"scorecard.md\", \"scorecard\", \"text/markdown\", scorecard_markdown(state))\n",
    "        write_artifact(self.s, self.run.id, \"graph_trace.jsonl\", \"trace\", \"application/json\", trace_jsonl(state))\n",
    "\n",
    "        self._update_status(RunStatus.succeeded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ed902",
   "metadata": {},
   "source": [
    "#### Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc66b9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/core/queue.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/core/queue.py\n",
    "# backend/app/core/queue.py\n",
    "import os\n",
    "import asyncio\n",
    "from arq import create_pool\n",
    "from arq.connections import RedisSettings\n",
    "from sqlalchemy.orm import Session\n",
    "from backend.app.storage.db import get_session, ensure_dirs\n",
    "from backend.app.storage.models import Run, RunStatus\n",
    "from backend.app.core.run_manager import RunManager\n",
    "\n",
    "REDIS_URL = os.environ.get(\"REDIS_URL\", \"redis://host.docker.internal:6379/0\")\n",
    "\n",
    "async def enqueue_run(run_id: str):\n",
    "    redis = await create_pool(RedisSettings.from_dsn(REDIS_URL))\n",
    "    await redis.enqueue_job(\"run_match_job\", run_id=run_id)\n",
    "\n",
    "async def run_match_job(ctx, run_id: str):\n",
    "    ensure_dirs()\n",
    "    # Do all heavy work here (worker process with its own loop)\n",
    "    with get_session() as s:\n",
    "        run = s.get(Run, run_id)\n",
    "        if not run:\n",
    "            return\n",
    "        if run.status not in (RunStatus.queued, RunStatus.failed):\n",
    "            return\n",
    "        mgr = RunManager(s, run)\n",
    "        try:\n",
    "            mgr.execute()\n",
    "        except Exception as e:\n",
    "            run.status = RunStatus.failed\n",
    "            run.error = str(e)\n",
    "            s.add(run)\n",
    "            s.commit()\n",
    "\n",
    "class WorkerSettings:\n",
    "    redis_settings = RedisSettings.from_dsn(REDIS_URL)\n",
    "    functions = [run_match_job]\n",
    "    max_jobs = 10\n",
    "    retry_jobs = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b142dc1",
   "metadata": {},
   "source": [
    "### Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be9245ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/worker.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/worker.py\n",
    "# backend/worker.py\n",
    "\n",
    "# Convenient entrypoint to run worker without module path issues\n",
    "from typing import cast\n",
    "from arq import run_worker\n",
    "from arq.typing import WorkerSettingsType\n",
    "from backend.app.core.queue import WorkerSettings\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_worker(cast(WorkerSettingsType, WorkerSettings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee7a877",
   "metadata": {},
   "source": [
    "### Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "112765ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/models/schemas.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/models/schemas.py\n",
    "# backend/app/models/schemas.py\n",
    "\n",
    "from typing import Optional, Dict, List, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class RunRequest(BaseModel):\n",
    "    resume_text: str = Field(..., min_length=1, description=\"Plain text resume\")\n",
    "    jd_text: str = Field(..., min_length=1, description=\"Plain text job description\")\n",
    "    params: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class RunResponse(BaseModel):\n",
    "    run_id: str\n",
    "    status: str\n",
    "\n",
    "class ArtifactMeta(BaseModel):\n",
    "    name: str\n",
    "    kind: str\n",
    "    mime: str\n",
    "\n",
    "class RunStatusResponse(BaseModel):\n",
    "    run_id: str\n",
    "    status: str\n",
    "    error: Optional[str] = None\n",
    "    artifacts: List[ArtifactMeta] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e120151",
   "metadata": {},
   "source": [
    "### API Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0c5d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/api/routes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/api/routes.py\n",
    "# backend/app/api/routes.py\n",
    "import json\n",
    "import hashlib\n",
    "from typing import Optional\n",
    "\n",
    "from fastapi import APIRouter, HTTPException\n",
    "from sqlalchemy import select\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from backend.app.models.schemas import RunRequest, RunResponse, RunStatusResponse, ArtifactMeta\n",
    "from backend.app.storage.db import get_session, ensure_dirs\n",
    "from backend.app.storage.models import Run, RunStatus\n",
    "from backend.app.core.queue import enqueue_run\n",
    "from backend.app.storage.artifacts import list_artifacts_for_run\n",
    "from fastapi.responses import FileResponse\n",
    "from backend.app.storage.artifacts import run_dir\n",
    "\n",
    "\n",
    "router = APIRouter()\n",
    "\n",
    "\n",
    "@router.post(\"/runs\", response_model=RunResponse, status_code=202)\n",
    "async def create_run(req: RunRequest) -> RunResponse:\n",
    "    \"\"\"Queue a new run. FastAPI will validate the body into RunRequest automatically.\"\"\"\n",
    "    ensure_dirs()\n",
    "\n",
    "    # Simple idempotency hash\n",
    "    h = hashlib.sha256(\n",
    "        (\n",
    "            req.resume_text.strip()\n",
    "            + \"\\n---\\n\"\n",
    "            + req.jd_text.strip()\n",
    "            + json.dumps(req.params or {}, sort_keys=True)\n",
    "        ).encode(\"utf-8\")\n",
    "    ).hexdigest()\n",
    "\n",
    "    with get_session() as s:\n",
    "        existing = (\n",
    "            s.execute(\n",
    "                select(Run)\n",
    "                .where(Run.payload_hash == h, Run.status == RunStatus.succeeded)\n",
    "                .order_by(Run.finished_at.desc())   # take newest\n",
    "            )\n",
    "            .scalars()\n",
    "            .first()\n",
    "        )\n",
    "        if existing:\n",
    "            return RunResponse(run_id=existing.id, status=existing.status.value)\n",
    "\n",
    "        run = Run(\n",
    "            payload_hash=h,\n",
    "            status=RunStatus.queued,\n",
    "            resume_text=req.resume_text,\n",
    "            jd_text=req.jd_text,\n",
    "            params=req.params or {},\n",
    "        )\n",
    "        s.add(run)\n",
    "        s.commit()\n",
    "        s.refresh(run)\n",
    "\n",
    "    # enqueue the job\n",
    "    await enqueue_run(run_id=run.id)\n",
    "\n",
    "    return RunResponse(run_id=run.id, status=str(RunStatus.queued.value))\n",
    "\n",
    "\n",
    "@router.get(\"/runs/{run_id}\", response_model=RunStatusResponse)\n",
    "async def get_run(run_id: str) -> RunStatusResponse:\n",
    "    with get_session() as s:\n",
    "        run = s.get(Run, run_id)\n",
    "        if not run:\n",
    "            raise HTTPException(status_code=404, detail=\"run not found\")\n",
    "\n",
    "        artifacts = []\n",
    "        if run.status == RunStatus.succeeded:\n",
    "            artifacts = [\n",
    "                ArtifactMeta(name=a.name, kind=a.kind, mime=a.mime)\n",
    "                for a in list_artifacts_for_run(s, run_id)\n",
    "            ]\n",
    "\n",
    "        return RunStatusResponse(\n",
    "            run_id=run.id, status=str(run.status.value), error=run.error, artifacts=artifacts\n",
    "        )\n",
    "\n",
    "\n",
    "@router.get(\"/artifacts/{run_id}\")\n",
    "async def list_artifacts(run_id: str):\n",
    "    with get_session() as s:\n",
    "        run = s.get(Run, run_id)\n",
    "        if not run:\n",
    "            raise HTTPException(status_code=404, detail=\"run not found\")\n",
    "        files = [a.name for a in list_artifacts_for_run(s, run_id)]\n",
    "        return {\"run_id\": run_id, \"files\": files}\n",
    "\n",
    "\n",
    "@router.get(\"/artifacts/{run_id}/{name}\")\n",
    "async def get_artifact(run_id: str, name: str):\n",
    "    with get_session() as s:\n",
    "        run = s.get(Run, run_id)\n",
    "        if not run:\n",
    "            raise HTTPException(status_code=404, detail=\"run not found\")\n",
    "        arts = list_artifacts_for_run(s, run_id)\n",
    "        for a in arts:\n",
    "            if a.name == name:\n",
    "                return FileResponse(a.path, media_type=a.mime, filename=a.name)\n",
    "    raise HTTPException(status_code=404, detail=\"artifact not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f6a465",
   "metadata": {},
   "source": [
    "### Main app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3bee2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/main.py\n",
    "# backend/app/main.py\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from backend.app.api.routes import router\n",
    "\n",
    "app = FastAPI(title=\"JobMatch-AI API\", version=\"0.1.0\")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"]\n",
    ")\n",
    "\n",
    "@app.get(\"/healthz\")\n",
    "async def healtz():\n",
    "    return {\"ok\": True}\n",
    "\n",
    "app.include_router(router, prefix=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829f36a",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674042b9",
   "metadata": {},
   "source": [
    "#### ConfTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb572b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../tests/conftest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../tests/conftest.py\n",
    "# tests/conftest.py\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pytest\n",
    "\n",
    "# Add repo root to PYTHONPATH for tests\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "# Force AnyIO to use asyncio to avoid trio dependency\n",
    "@pytest.fixture\n",
    "def anyio_backend():\n",
    "    return \"asyncio\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc1f1de",
   "metadata": {},
   "source": [
    "#### Api Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c8af68",
   "metadata": {},
   "source": [
    "##### W/O Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b153c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../tests/test_api_contract.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../tests/test_api_contract.py\n",
    "# tests/test_api_contract.py\n",
    "import pytest\n",
    "import uuid\n",
    "from httpx import AsyncClient, ASGITransport\n",
    "from backend.app.main import app\n",
    "from backend.app.storage.db import ensure_dirs\n",
    "\n",
    "\n",
    "@pytest.mark.anyio\n",
    "async def test_healthz():\n",
    "    transport = ASGITransport(app=app)\n",
    "    async with AsyncClient(transport=transport, base_url=\"http://test\") as ac:\n",
    "        r = await ac.get(\"/healthz\")\n",
    "        assert r.status_code == 200\n",
    "        assert r.json()[\"ok\"] is True\n",
    "\n",
    "\n",
    "@pytest.mark.anyio\n",
    "async def test_create_and_poll_run(monkeypatch):\n",
    "    ensure_dirs()\n",
    "\n",
    "    async def _noop_enqueue_run(*args, **kwargs):\n",
    "        return None\n",
    "\n",
    "    import backend.app.api.routes as routes_mod\n",
    "    monkeypatch.setattr(routes_mod, \"enqueue_run\", _noop_enqueue_run)\n",
    "\n",
    "    transport = ASGITransport(app=app)\n",
    "    async with AsyncClient(transport=transport, base_url=\"http://test\") as ac:\n",
    "        payload = {\n",
    "            \"resume_text\": f\"A B C {uuid.uuid4()}\",\n",
    "            \"jd_text\": \"A B\",\n",
    "            \"params\": {}\n",
    "        }\n",
    "        r = await ac.post(\"/runs\", json=payload)\n",
    "        assert r.status_code in (200, 202)\n",
    "        run_id = r.json()[\"run_id\"]\n",
    "\n",
    "        r2 = await ac.get(f\"/runs/{run_id}\")\n",
    "        assert r2.status_code == 200\n",
    "        body = r2.json()\n",
    "        assert body[\"run_id\"] == run_id\n",
    "        assert body[\"status\"] in (\"queued\", \"running\", \"succeeded\", \"failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73b128",
   "metadata": {},
   "source": [
    "##### With Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afc88ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../tests/test_api_contract.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../tests/test_api_contract.py\n",
    "# tests/test_api_contract.py\n",
    "import pytest\n",
    "from httpx import AsyncClient\n",
    "from httpx import ASGITransport \n",
    "from backend.app.main import app\n",
    "from backend.app.storage.db import ensure_dirs\n",
    "\n",
    "@pytest.mark.anyio\n",
    "async def test_healthz():\n",
    "    transport = ASGITransport(app=app)  # NEW\n",
    "    async with AsyncClient(transport=transport, base_url=\"http://test\") as ac:\n",
    "        r = await ac.get(\"/healthz\")\n",
    "        assert r.status_code == 200\n",
    "        assert r.json()[\"ok\"] is True\n",
    "\n",
    "@pytest.mark.anyio\n",
    "async def test_create_and_poll_run(monkeypatch):\n",
    "    ensure_dirs()\n",
    "    transport = ASGITransport(app=app)  # NEW\n",
    "    async with AsyncClient(transport=transport, base_url=\"http://test\") as ac:\n",
    "        r = await ac.post(\"/runs\", json={\"resume_text\": \"A B C\", \"jd_text\": \"A B\", \"params\": {}})\n",
    "        assert r.status_code in (200, 202)\n",
    "        run_id = r.json()[\"run_id\"]\n",
    "\n",
    "        r2 = await ac.get(f\"/runs/{run_id}\")\n",
    "        assert r2.status_code == 200\n",
    "        body = r2.json()\n",
    "        assert body[\"run_id\"] == run_id\n",
    "        assert body[\"status\"] in (\"queued\", \"running\", \"succeeded\", \"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c8a8b",
   "metadata": {},
   "source": [
    "#### Test graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeaa681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../tests/test_graph_unit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../tests/test_graph_unit.py\n",
    "# tests/test_graph_unit.py\n",
    "\n",
    "from backend.app.core.graph import run_minimal_graph\n",
    "\n",
    "def test_graph_basic():\n",
    "    resume = \"Built APIs in Python & FastAPI. Used PostgreSQL and Docker. Deployed on AWS.\"\n",
    "    jd = \"Looking for a Python developer with FastAPI, PostgreSQL, and AWS experience.\"\n",
    "    st = run_minimal_graph(resume, jd)\n",
    "    assert st.scorecard[\"overall_score\"] > 50\n",
    "    assert \"python\" in st.skills_resume\n",
    "    assert \"fastapi\" in st.skills_resume\n",
    "    assert st.coverage >= 50.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebae484f",
   "metadata": {},
   "source": [
    "## **`Frontend`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7d605",
   "metadata": {},
   "source": [
    "### Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edaa600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../frontend/streamlit_app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../frontend/streamlit_app/main.py\n",
    "# streamlit_app/main.py\n",
    "import os\n",
    "import asyncio\n",
    "import httpx\n",
    "import streamlit as st\n",
    "\n",
    "API_BASE = os.environ.get(\"API_BASE\", \"http://localhost:8000\")\n",
    "\n",
    "st.set_page_config(page_title=\"JobMatch-AI\", layout=\"wide\")\n",
    "st.title(\"JobMatch-AI — Resume ↔ JD Matcher\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.markdown(\"**Status**: Test (stub graph)\")\n",
    "    st.markdown(\"API: \" + API_BASE)\n",
    "\n",
    "resume_text = st.text_area(\"Paste your Resume (plain text)\", height=220)\n",
    "jd_text = st.text_area(\"Paste the Job Description (plain text)\", height=220)\n",
    "params_col1, params_col2 = st.columns(2)\n",
    "with params_col1:\n",
    "    threshold = st.slider(\"Minimum score threshold (UI only)\", 0, 100, 65)\n",
    "\n",
    "run_btn = st.button(\"Run Match\", type=\"primary\", use_container_width=True)\n",
    "\n",
    "status_placeholder = st.empty()\n",
    "artifacts_placeholder = st.empty()\n",
    "\n",
    "async def poll_status(run_id: str):\n",
    "    status_placeholder.info(f\"Run `{run_id}` queued…\")\n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        while True:\n",
    "            r = await client.get(f\"{API_BASE}/runs/{run_id}\")\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            if data[\"status\"] == \"succeeded\":\n",
    "                status_placeholder.success(f\"Run `{run_id}`: succeeded ✅\")\n",
    "                return data\n",
    "            if data[\"status\"] == \"failed\":\n",
    "                status_placeholder.error(f\"Run `{run_id}`: failed ❌ — {data.get('error')}\")\n",
    "                return data\n",
    "            status_placeholder.info(f\"Run `{run_id}`: {data['status']}…\")\n",
    "            await asyncio.sleep(1.0)\n",
    "\n",
    "if run_btn:\n",
    "    if not resume_text.strip() or not jd_text.strip():\n",
    "        st.warning(\"Please paste both Resume and Job Description.\")\n",
    "    else:\n",
    "        with st.spinner(\"Submitting…\"):\n",
    "            resp = httpx.post(f\"{API_BASE}/runs\", json={\"resume_text\": resume_text, \"jd_text\": jd_text, \"params\": {\"ui_threshold\": threshold}})\n",
    "            if resp.status_code not in (200, 202):\n",
    "                st.error(f\"Error: {resp.text}\")\n",
    "            else:\n",
    "                run_id = resp.json()[\"run_id\"]\n",
    "                data = asyncio.run(poll_status(run_id))\n",
    "                if data and data.get(\"artifacts\"):\n",
    "                    with artifacts_placeholder.container():\n",
    "                        st.subheader(\"Artifacts\")\n",
    "                        for a in data[\"artifacts\"]:\n",
    "                            url = f\"{API_BASE}/artifacts/{run_id}/{a['name']}\"\n",
    "                            st.write(f\"- **{a['name']}** ({a['mime']}) — [download]({url})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c718505b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-jd-matcher-langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
